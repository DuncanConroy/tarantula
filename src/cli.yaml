name: tarantula
version: "1.0"
author: Daniel Bunte <daniel.bunte.84@gmail.com>
about: Crawls Websites
args:
  - ignore_redirects:
      long: ignore_redirects
      help: Ignore redirects; default=false
  - ignore_robots_txt:
      long: ignore_robots_txt
      help: Ignores entries in the robots.txt file. This might not be compliant with local law! ; default=false
  - keep_html_in_memory:
      short: k
      long: keep_html_in_memory
      help: Keeps all HTML sources in memory. Will grow mem usage to several GB, depending on URL-complexity. Might lead to OOM errors. ; default=false
  - maximum_depth:
      long: maximum_depth
      help: Maximum depth of traversing nested links; default=16
      takes_value: true
  - maximum_redirects:
      long: maximum_redirects
      help: Maximum redirects we will follow (to prevent endless redirects)of traversing nested links; default=10
      takes_value: true
  - URL:
      help: Sets the url to crawl, e.g. https://www.example.com
      required: true
      index: 1
#  - verbose:
#      short: v
#      multiple: true
#      help: Sets the level of verbosity
#subcommands:
#  - test:
#      about: controls testing features
#      version: "1.3"
#      author: Someone E. <someone_else@other.com>
#      args:
#        - debug:
#            short: d
#            help: print debug information